# ollama-local-llm-server
Ollama Local LLM Server is a one-click solution to run Ollama-powered large language models locally with Node.js and WebSocket support. This project provides a lightweight server.js setup that enables real-time communication between clients and your local AI model.
