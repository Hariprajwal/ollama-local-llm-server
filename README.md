# ollama-local-llm-server
Ollama Local LLM Server is a one-click solution to run Ollama-powered large language models locally with Node.js and WebSocket support. This project provides a lightweight server.js setup that enables real-time communication between clients and your local AI model.
<img width="1918" height="924" alt="image" src="https://github.com/user-attachments/assets/776c7d5f-286e-4910-a8a7-924432432398" />

<img width="1468" height="731" alt="image" src="https://github.com/user-attachments/assets/9dd17436-1106-4298-9a0d-d329f92fcaf2" />
