# ollama-local-llm-server
Ollama Local LLM Server is a one-click solution to run Ollama-powered large language models locally with Node.js and WebSocket support. This project provides a lightweight server.js setup that enables real-time communication between clients and your local AI model.

<img width="1918" height="924" alt="image" src="https://github.com/user-attachments/assets/776c7d5f-286e-4910-a8a7-924432432398" />
<img width="1482" height="705" alt="image" src="https://github.com/user-attachments/assets/025820db-2000-45f6-a7b5-93c32e650e91" />


